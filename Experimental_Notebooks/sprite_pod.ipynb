{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALGORITHM\n",
    "\n",
    "TRAINING\n",
    "1. Generate a noisy sprite\n",
    "2. Choose the closest goal sprite from the training data (hamming distance)\n",
    "3. Destroy the goal sprite (sequentially over pixles) until it matches the noisy sprite\n",
    "4. Add the level repair step to the training data\n",
    "5. Repeat until it matches the noise\n",
    "\n",
    "---\n",
    "\n",
    "GENERATION \n",
    "1. Initialize a noisy sprite\n",
    "2. Randomly or sequentially pick/fix a pixel\n",
    "3. Feed the level into the network to output a repair value\n",
    "4. Update the pixel with the value\n",
    "5. If the current sprite isn't valid, keep going"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from matplotlib.colors import ListedColormap\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import InputLayer, Dense, Conv2D, Conv2DTranspose, Flatten, Layer, Reshape, Input, LeakyReLU, MaxPooling2D, Concatenate, Conv1D, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/milk/Desktop/GIL_Lab/BMO/BMO_chatbot_prototype/bmo-venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#import from Python folder to get the utils\n",
    "import sys\n",
    "sys.path.append('../Python')\n",
    "from utils import picoSS2np, showMultiSprPalette, showMultiSprRGB, showSprRGB, showSprPalette, animatePal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTANT VARIABLES   ###\n",
    "\n",
    "# PICO-8 Palette => use colormap for matplotlib\n",
    "PICO_PALETTE = ['#000000','#1D2B53','#7E2553','#008751','#AB5236','#5F574F','#C2C3C7','#FFF1E8','#FF004D','#FFA300','#FFEC27','#00E436','#29ADFF','#83769C','#FF77A8','#FFCCAA']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Food Dataset shape: (100, 8, 8)\n",
      "Char Dataset shape: (100, 8, 8)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import from the PICO sprites posted onto Twitter \n",
    "# Food: https://twitter.com/JUSTIN_CYR/status/634546317713391616\n",
    "# Characters: https://twitter.com/johanvinet/status/635814153601597441\n",
    "pico_food_dat = np.load('../data/rip_data/pico_food.npy',allow_pickle=True)\n",
    "pico_char_dat = np.load('../data/rip_data/pico_characters.npy',allow_pickle=True)\n",
    "\n",
    "print(f\"Food Dataset shape: {pico_food_dat.shape}\")\n",
    "print(f\"Char Dataset shape: {pico_char_dat.shape}\")\n",
    "\n",
    "# show a random food and character sprite \n",
    "rand_food = random.choice(pico_food_dat)\n",
    "rand_char = random.choice(pico_char_dat)\n",
    "showMultiSprPalette([rand_food,rand_char],textArr=['Food','Character'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding labels: 100%|██████████| 100/100 [00:07<00:00, 13.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# import sentence-transformer for text embedding \n",
    "SBERT_MODEL = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "def sentEmb(txt):\n",
    "    return SBERT_MODEL.encode([txt])[0]\n",
    "\n",
    "# import the text data\n",
    "pico_char_labels = np.array([l.strip() for l in open('../data/rip_data/character_desc.txt','r').readlines()])\n",
    "\n",
    "# encode the labels\n",
    "pico_char_labels_emb = []\n",
    "with tqdm(total=len(pico_char_labels)) as pbar:\n",
    "    pbar.set_description(\"Encoding labels\")\n",
    "    for l in pico_char_labels:\n",
    "        pico_char_labels_emb.append(sentEmb(l))\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character: a large yellow turtle with red hair and a large mouth\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAK4AAACuCAYAAACvDDbuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAAADKklEQVR4nO3dIa9XdRzH8XMc3UIwwEZxPAM2gnQLG8KEoMFAMFlIBDYlMBLNJ0DjXuamD4HAhnSDhWDwbhYtNg+ZDbcz9tX/fbPXK999zj+89yt3v/3Wbdu2BWI+OPQPgHchXJKES5JwSRIuScIlSbgkCZck4ZJ0Zu8fruv6X/6Od7b9+fvs4Ie3Z/eGrMtPY1v3Pr4+trUsy/Ldr0/Htvb+I9eJS5JwSRIuScIlSbgkCZck4ZIkXJKES5JwSRIuScIlSbgkCZck4ZIkXJKES5JwSdp9dWfUg2sH+ewe628nc2PnXoxNbS+fjG1N+3a5+b9/04lLknBJEi5JwiVJuCQJlyThkiRckoRLknBJEi5JwiVJuCQJlyThkiRckoRLknBJ2n11Z/J1m/X7r8e2jr+6Ora1LMuy3b0zN/Zyboo3OXFJEi5JwiVJuCQJlyThkiRckoRLknBJEi5JwiVJuCQJlyThkiRckoRLknBJEi5JwiVp952zyXti27kfxraOf740tnWqHf1z6F/wr9aHt8a2tm3b9XdOXJKES5JwSRIuScIlSbgkCZck4ZIkXJKES5JwSRIuScIlSbgkCZck4ZIkXJKES9K67b0r8dfJ3Fd//Ghsav1ybGpZlmXZ/rg4N/bq/tzWafbL53NbX7i6w3tMuCQJlyThkiRckoRLknBJEi5JwiVJuCQJlyThkiRckoRLknBJEi5JwiVJuCQd5NWdo9GXcl4Mbs169vf5sa1PXl0e2xq/7vR4dm8PJy5JwiVJuCQJlyThkiRckoRLknBJEi5JwiVJuCQJlyThkiRckoRLknBJEi5JwiVp96s767qOffQQVz32enbh+aF/wludPPpmbOvGtdnrTpNXgfY+AuXEJUm4JAmXJOGSJFyShEuScEkSLknCJUm4JAmXJOGSJFyShEuScEkSLknCJUm4JAmXpN3PRZ1an14cnbtydu5ZpklHn00+sdXnxCVJuCQJlyThkiRckoRLknBJEi5JwiVJuCQJlyThkiRckoRLknBJEi5JwiVJuCTtfi4KThMnLknCJUm4JAmXJOGSJFyShEuScEkSLkmvAegGUeo/jr35AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding length: 768\n"
     ]
    }
   ],
   "source": [
    "# show a random character and its description\n",
    "rand_char_idx = random.randint(0,len(pico_char_labels))\n",
    "print(f\"Character: {pico_char_labels[rand_char_idx]}\")\n",
    "showSprPalette(pico_char_dat[rand_char_idx])\n",
    "\n",
    "desc_enc = pico_char_labels_emb[rand_char_idx]\n",
    "print(f\"Encoding length: {len(desc_enc)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   CONFIUGURATIONS   ###\n",
    "\n",
    "EXPERIMENT = \"text_enc\"\n",
    "\n",
    "GEN_CONF = {\n",
    "    \"EPOCHS\" : 50,\n",
    "    \"BATCH_SIZE\" : 64,\n",
    "    \"LEARNING_RATE\" : 0.0001,\n",
    "    \"WINDOW\" : 8\n",
    "}\n",
    "\n",
    "CONFIGS = {\n",
    "    \"normal\": {\n",
    "        \"NEW_TRAIN_DAT\" : True,\n",
    "        \"TRAIN_DAT_PATH\" : f'../data/rip_data/POD_dat/POD_train_dat_w{GEN_CONF[\"WINDOW\"]}.npy',\n",
    "        \"TRAIN_NPOD\" : False,\n",
    "        \"NPOD_MODEL\" : f\"../models/gen_models/pod/npod_char-{GEN_CONF['EPOCHS']}eR_w{GEN_CONF['WINDOW']}.h5\",\n",
    "        \"SANITY\" : False\n",
    "    },\n",
    "    \"text_enc\": {\n",
    "        \"NEW_TRAIN_DAT\" : False,\n",
    "        \"TRAIN_DAT_PATH\" : f'../data/rip_data/POD_dat/tePOD_train_dat_w{GEN_CONF[\"WINDOW\"]}.npy',\n",
    "        \"TRAIN_TEPOD\" : False,\n",
    "        \"TEPOD_MODEL\" : f\"../models/gen_models/pod/tepod_char-{GEN_CONF['EPOCHS']}eR_w{GEN_CONF['WINDOW']}.h5\",\n",
    "        \"SANITY\" : False\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal PoD\n",
    "\n",
    "---\n",
    "\n",
    "#### Notes\n",
    "\n",
    "[window size: 5]\n",
    "[epochs: 50]\n",
    "- Abstract art-esque - not quite character looking though\n",
    "\n",
    "[window size: 8]\n",
    "[epochs: 50]\n",
    "- More character definition. Much more cohesive\n",
    "\n",
    "[window size: 9]\n",
    "[epochs: 50]\n",
    "- Not much change but makes a 'character' at earlier iterations\n",
    "\n",
    "[window size: 9 - TMNT edition]\n",
    "[epochs: 10 (reached 98%)]\n",
    "- perfectly recreates ninja turtles every time; can replace a tile with the same tile\n",
    "\n",
    "[window size: 4]\n",
    "[epochs: 50]\n",
    "- \"there is a SHAPE\" - Dipika\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_27 (Conv2D)          (None, 4, 4, 128)         1280      \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 2, 2, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_28 (Conv2D)          (None, 2, 2, 128)         147584    \n",
      "                                                                 \n",
      " conv2d_29 (Conv2D)          (None, 2, 2, 256)         295168    \n",
      "                                                                 \n",
      " flatten_9 (Flatten)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 128)               131200    \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 16)                2064      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 577,296\n",
      "Trainable params: 577,296\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class NormalPoD():\n",
    "    def __init__(self,spr_shape, channels=16, pad_val=0, crop_size=5):\n",
    "        self.destroy_data = []\n",
    "        self.spr_shape = spr_shape\n",
    "        self.channels = channels\n",
    "        self.pad_val = pad_val\n",
    "        self.crop_size = crop_size\n",
    "\n",
    "    # make a new noisy sprite using the channels and size\n",
    "    def init_noise_sprite(self):\n",
    "        noise_spr = np.random.randint(self.channels,size=(self.spr_shape[0],self.spr_shape[1]))\n",
    "        return noise_spr\n",
    "\n",
    "    # crop a sprite from the noise sprite (use with padding) - assume pos is (x,y) and center of target crop\n",
    "    def crop(self, spr, pos, size=5):\n",
    "        hpad = size//2\n",
    "        pad_spr = np.pad(spr,((hpad,hpad),(hpad,hpad)),constant_values=self.pad_val)   # pad the sprite \n",
    "        crop_spr = pad_spr[pos[0]:pos[0]+size,pos[1]:pos[1]+size]   # get the cropped sprite (should be size,size)\n",
    "        return pad_spr, crop_spr\n",
    "\n",
    "    # return the new level and the tile replaced (also add to training data) - assume pos is (x,y)\n",
    "    def destroy(self, cur_spr, bad_spr, pos, crop_size=5):\n",
    "        # turn the cur_spr value at the pos to the bad_spr value\n",
    "        cur_spr2 = np.copy(cur_spr)\n",
    "        og_val = cur_spr[pos[0],pos[1]]\n",
    "        cur_spr2[pos[0],pos[1]] = bad_spr[pos[0],pos[1]]\n",
    "\n",
    "        # crop around the area and add to training data\n",
    "        _, crop_spr = self.crop(cur_spr2,pos,crop_size)\n",
    "\n",
    "        return cur_spr2, og_val, crop_spr\n",
    "    \n",
    "    # calculate the hamming distance from 2 sprites\n",
    "    def calc_ham_dist(self, spr1, spr2):\n",
    "        return np.sum(spr1 != spr2)\n",
    "\n",
    "    # return the sprite that is closest in hamming distance from the goal set\n",
    "    def closest_spr(self, goal_set, noise_spr):\n",
    "        min_dist_i = np.argmin([self.calc_ham_dist(noise_spr, i) for i in goal_set])\n",
    "        return goal_set[min_dist_i]\n",
    "\n",
    "    \n",
    "    # make the training data for the path of destruction\n",
    "    def make_train_dat(self,goal_set,num_noise_spr=1000):\n",
    "        train_dat = []\n",
    "        with tqdm(total=num_noise_spr) as pbar:\n",
    "            for i in range(num_noise_spr):\n",
    "                # make a noise sprite and find the closest\n",
    "                # noise_spr = self.init_noise_sprite()\n",
    "                # targ_spr = self.closest_spr(goal_set,noise_spr).copy()\n",
    "\n",
    "                #go through each goal sprite and make a noise sprite for it\n",
    "                targ_spr = goal_set[i%len(goal_set)]\n",
    "                noise_spr = self.init_noise_sprite()\n",
    "\n",
    "                # make a list of all the positions in the sprite and shuffle it\n",
    "                pos_set = []\n",
    "                for x in range(self.spr_shape[0]):\n",
    "                    for y in range(self.spr_shape[1]):\n",
    "                        pos_set.append((x,y))\n",
    "                random.shuffle(pos_set)\n",
    "\n",
    "                # iterate over the goal sprite and change until it matches the noise spr (destroy)\n",
    "                for p in pos_set:\n",
    "                        # destroy the noise sprite\n",
    "                        targ_spr, og_val, crop_spr = self.destroy(targ_spr,noise_spr,p,self.crop_size)\n",
    "                        train_dat.append([crop_spr,og_val])\n",
    "                        # showSprPalette(targ_spr)\n",
    "                pbar.update(1)\n",
    "                    \n",
    "        return train_dat\n",
    "    \n",
    "    # make the path of destruction neural network model\n",
    "    # copy of architecture from the paper [Conv(128,3) -> Conv(128,3) -> MaxPool(2), Conv(256,3)]\n",
    "    # it doesn't work though... at least not with 5x5 crop size - need to ask Sam for exact implementation\n",
    "    def makePoDCNN(self):\n",
    "        self.pod_model = Sequential([\n",
    "            InputLayer(input_shape=(self.crop_size,self.crop_size,1)),\n",
    "            Conv2D(128, (3, 3), activation='relu',padding='SAME'),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            Conv2D(128, (3, 3), activation='relu',padding='SAME'),\n",
    "            Conv2D(256, (3, 3), activation='relu',padding='SAME'),\n",
    "            Flatten(),\n",
    "            Dense(128),\n",
    "            Dense(self.channels,activation='softmax'),\n",
    "        ])\n",
    "        self.pod_model.summary()\n",
    "        self.opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "        self.pod_model.compile(optimizer=self.opt,metrics=['accuracy'],loss='sparse_categorical_crossentropy') #not one-hot encoded\n",
    "        \n",
    "        \n",
    "    # import a model from a path\n",
    "    def importModel(self,model_path):\n",
    "        self.pod_model = tf.keras.models.load_model(model_path)\n",
    "        self.pod_model.summary()\n",
    "        self.opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "        self.pod_model.compile(optimizer=self.opt,metrics=['accuracy'],loss='sparse_categorical_crossentropy')\n",
    "\n",
    "\n",
    "    # export the model to a path\n",
    "    def exportModel(self,model_path):\n",
    "        self.pod_model.save(model_path,save_format='h5')\n",
    "                            \n",
    "\n",
    "    # train the path of destruction model on the training data\n",
    "    def trainPoD(self,train_dat,EPOCHS=500,BATCH_SIZE=64,show_acc=False):\n",
    "        # train the model\n",
    "        X = np.array([i[0] for i in train_dat])\n",
    "        Y = np.array([i[1] for i in train_dat])\n",
    "        X = np.expand_dims(X,axis=-1)\n",
    "        Y = np.expand_dims(Y,axis=-1)\n",
    "        print(X.shape,Y.shape)\n",
    "\n",
    "        h = self.pod_model.fit(X,Y,epochs=EPOCHS,batch_size=BATCH_SIZE,shuffle=True)\n",
    "        if show_acc:\n",
    "            plt.plot(h.history['accuracy'])\n",
    "            plt.show()\n",
    "\n",
    "    # repair from a noise vector\n",
    "    def repair(self,init_spr=None,mod_iter='rand',eval_met='iter',num_iter=1000,animate=False):\n",
    "        # make a noise sprite\n",
    "        if init_spr is None:\n",
    "            init_spr = self.init_noise_sprite()\n",
    "        cur_spr = np.copy(init_spr)\n",
    "\n",
    "        if animate:\n",
    "            anim_set = []\n",
    "            anim_set.append(cur_spr)\n",
    "\n",
    "        # iterate over the noise sprite and change until the iterations are done\n",
    "        pi = 0\n",
    "        if eval_met == \"iter\":\n",
    "            with tqdm(total=num_iter) as pbar:\n",
    "                for i in range(num_iter):\n",
    "                    #select a position (randomly or sequentially)\n",
    "                    if mod_iter == \"rand\":\n",
    "                        x = np.random.randint(self.spr_shape[0])\n",
    "                        y = np.random.randint(self.spr_shape[1])\n",
    "                    else:\n",
    "                        x = pi%self.spr_shape[0]\n",
    "                        y = pi//self.spr_shape[0]\n",
    "                        pi += 1\n",
    "                        if pi >= self.spr_shape[0]*self.spr_shape[1]:\n",
    "                            pi = 0\n",
    "\n",
    "                    pbar.set_description(\"@ ({},{})\".format(x,y))\n",
    "\n",
    "                    #crop the area\n",
    "                    _, crop_spr = self.crop(cur_spr,(x,y),self.crop_size)\n",
    "                    \n",
    "                    # get the pixel change input from the model\n",
    "                    pred_px = self.pod_model.predict(np.array([crop_spr]),verbose=False)\n",
    "\n",
    "                    # change the tile\n",
    "                    cur_spr[x,y] = np.argmax(pred_px)\n",
    "\n",
    "                    anim_set.append(cur_spr.copy())\n",
    "\n",
    "                    pbar.update(1)\n",
    "\n",
    "        if animate:\n",
    "            return init_spr, cur_spr, anim_set\n",
    "        return init_spr, cur_spr\n",
    "\n",
    "if EXPERIMENT == \"normal\":\n",
    "    # Make the Path of Destruction  model\n",
    "    npod = NormalPoD((8,8),channels=16,crop_size=GEN_CONF['WINDOW'])     \n",
    "    npod.makePoDCNN() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 282.38it/s]\n",
      "/Users/milk/Desktop/GIL_Lab/BMO/BMO_chatbot_prototype/bmo-venv/lib/python3.8/site-packages/numpy/lib/npyio.py:501: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.asanyarray(arr)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64000\n"
     ]
    }
   ],
   "source": [
    "if EXPERIMENT == \"normal\":\n",
    "\n",
    "    if CONFIGS[\"normal\"][\"NEW_TRAIN_DAT\"]:\n",
    "        # make the training data\n",
    "        goal_spr_char = pico_char_dat.copy()\n",
    "        train_dat = npod.make_train_dat(goal_spr_char,num_noise_spr=(len(goal_spr_char)*10))\n",
    "\n",
    "        # ninja turtle set\n",
    "        # turtle_spr_char = pico_char_dat[30:34]\n",
    "        # showMultiSprPalette(turtle_spr_char)\n",
    "        # train_dat = npod.make_train_dat(turtle_spr_char,num_noise_spr=(len(turtle_spr_char)*100))\n",
    "\n",
    "        print(len(train_dat))\n",
    "\n",
    "        # export as .npy file\n",
    "        np.save(CONFIGS[\"normal\"][\"TRAIN_DAT_PATH\"],train_dat)\n",
    "    else:\n",
    "        # import the training data\n",
    "        train_dat = np.load(CONFIGS['normal'][\"TRAIN_DAT_PATH\"],allow_pickle=True)\n",
    "        print(len(train_dat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 4, 4, 128)         1280      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 2, 2, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 2, 2, 128)         147584    \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 2, 2, 256)         295168    \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               131200    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 16)                2064      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 577,296\n",
      "Trainable params: 577,296\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# train or import the model\n",
    "if EXPERIMENT == \"normal\":\n",
    "    if CONFIGS[\"normal\"][\"TRAIN_NPOD\"]:\n",
    "        npod.trainPoD(train_dat,EPOCHS=GEN_CONF['EPOCHS'],BATCH_SIZE=GEN_CONF['BATCH_SIZE'],show_acc=True)\n",
    "        npod.exportModel(CONFIGS[\"normal\"][\"NPOD_MODEL\"])\n",
    "    else:\n",
    "        npod.importModel(CONFIGS[\"normal\"][\"NPOD_MODEL\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "@ (7,6): 100%|██████████| 1069/1069 [00:49<00:00, 21.74it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "saving GIF 1070...: 100%|██████████| 1070/1070 [00:53<00:00, 19.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# show the output from repairing a noise sprite\n",
    "if EXPERIMENT == \"normal\":\n",
    "    init_spr, cur_spr, anims = npod.repair(mod_iter='rand',num_iter=1069,animate=True)\n",
    "    showMultiSprPalette([init_spr,cur_spr],textArr=['init','final'])\n",
    "\n",
    "    # save the animation\n",
    "    animatePal(anims,f\"../prelim_output/pod_anim/npod_{GEN_CONF['EPOCHS']}e_w{GEN_CONF['WINDOW']}-2.gif\",fps=32,textArr=[f\"Iter: {i}\" for i in range(len(anims))])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NPOD Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 8)\n",
      "(2, 2)\n",
      "(8, 8)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x700 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if EXPERIMENT == \"normal\" and CONFIGS[\"normal\"][\"SANITY\"]:\n",
    "    # test destroy function\n",
    "    turt_spr = pico_char_dat[30]\n",
    "    n_spr = npod.init_noise_sprite()\n",
    "\n",
    "    pos = (4,4)\n",
    "    a,b,c = npod.destroy(turt_spr,n_spr,pos,crop_size=8)\n",
    "    b2d = np.full((2,2),b)\n",
    "    print(a.shape)\n",
    "    print(b2d.shape)\n",
    "    print(c.shape)\n",
    "\n",
    "    # showMultiSprPalette([turt_spr,n_spr], ['turt','noise'])\n",
    "    showMultiSprPalette([turt_spr,n_spr,a,b2d,c],textArr=['turt','noise','new turt','pixel','crop'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 288.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# test train data creation\n",
    "if EXPERIMENT == \"normal\" and CONFIGS[\"normal\"][\"SANITY\"]:\n",
    "    turt_spr = pico_char_dat[30]\n",
    "    tdat = npod.make_train_dat([turt_spr],num_noise_spr=1)\n",
    "    print(len(tdat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test output of the model\n",
    "if EXPERIMENT == \"normal\" and CONFIGS[\"normal\"][\"SANITY\"]:\n",
    "    turt_spr = pico_char_dat[0].copy()\n",
    "    turt_spr2 = turt_spr.copy()\n",
    "    turt_spr2[4,2] = 14\n",
    "    _,c = npod.crop(turt_spr2,(4,2),5)\n",
    "    print(c.shape)\n",
    "\n",
    "    #fix the sprite\n",
    "    new_px = npod.pod_model.predict(np.array([c]),verbose=False)\n",
    "    turt_spr3 = np.copy(turt_spr2)\n",
    "    turt_spr3[4,2] = np.argmax(new_px)\n",
    "\n",
    "    showMultiSprPalette([turt_spr,turt_spr2,turt_spr3],textArr=['OG','destroy','repair'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEXT ENCODED PoD\n",
    "\n",
    "---\n",
    "\n",
    "#### Notes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_21\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_120 (InputLayer)         [(None, 768)]        0           []                               \n",
      "                                                                                                  \n",
      " tf.reshape_40 (TFOpLambda)     (None, 8, 8, 12)     0           ['input_120[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_35 (Flatten)           (None, 768)          0           ['tf.reshape_40[1][0]']          \n",
      "                                                                                                  \n",
      " dense_72 (Dense)               (None, 64)           49216       ['flatten_35[1][0]']             \n",
      "                                                                                                  \n",
      " input_121 (InputLayer)         [(None, 8, 8, 1)]    0           []                               \n",
      "                                                                                                  \n",
      " tf.reshape_41 (TFOpLambda)     (None, 8, 8, 1)      0           ['dense_72[1][0]']               \n",
      "                                                                                                  \n",
      " concatenate_24 (Concatenate)   (None, 8, 8, 2)      0           ['input_121[0][0]',              \n",
      "                                                                  'tf.reshape_41[1][0]']          \n",
      "                                                                                                  \n",
      " conv2d_92 (Conv2D)             (None, 8, 8, 128)    2432        ['concatenate_24[1][0]']         \n",
      "                                                                                                  \n",
      " max_pooling2d_30 (MaxPooling2D  (None, 4, 4, 128)   0           ['conv2d_92[1][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_93 (Conv2D)             (None, 4, 4, 128)    147584      ['max_pooling2d_30[1][0]']       \n",
      "                                                                                                  \n",
      " conv2d_94 (Conv2D)             (None, 4, 4, 256)    295168      ['conv2d_93[1][0]']              \n",
      "                                                                                                  \n",
      " flatten_36 (Flatten)           (None, 4096)         0           ['conv2d_94[1][0]']              \n",
      "                                                                                                  \n",
      " dense_73 (Dense)               (None, 128)          524416      ['flatten_36[1][0]']             \n",
      "                                                                                                  \n",
      " dense_74 (Dense)               (None, 16)           2064        ['dense_73[1][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,020,880\n",
      "Trainable params: 1,020,880\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class TECPoD():\n",
    "    def __init__(self,spr_shape, channels=16, pad_val=0, crop_size=5):\n",
    "        self.destroy_data = []\n",
    "        self.spr_shape = spr_shape\n",
    "        self.channels = channels\n",
    "        self.pad_val = pad_val\n",
    "        self.crop_size = crop_size\n",
    "\n",
    "        self.ENC_SIZE = 768\n",
    "\n",
    "    # make a new noisy sprite using the channels and size\n",
    "    def init_noise_sprite(self):\n",
    "        noise_spr = np.random.randint(self.channels,size=(self.spr_shape[0],self.spr_shape[1]))\n",
    "        return noise_spr\n",
    "\n",
    "    # crop a sprite from the noise sprite (use with padding) - assume pos is (x,y) and center of target crop\n",
    "    def crop(self, spr, pos, size=5):\n",
    "        hpad = size//2\n",
    "        pad_spr = np.pad(spr,((hpad,hpad),(hpad,hpad)),constant_values=self.pad_val)   # pad the sprite \n",
    "        crop_spr = pad_spr[pos[0]:pos[0]+size,pos[1]:pos[1]+size]   # get the cropped sprite (should be size,size)\n",
    "        return pad_spr, crop_spr\n",
    "\n",
    "    # return the new level and the tile replaced (also add to training data) - assume pos is (x,y)\n",
    "    def destroy(self, cur_spr, bad_spr, pos, crop_size=5):\n",
    "        # turn the cur_spr value at the pos to the bad_spr value\n",
    "        cur_spr2 = np.copy(cur_spr)\n",
    "        og_val = cur_spr[pos[0],pos[1]]\n",
    "        cur_spr2[pos[0],pos[1]] = bad_spr[pos[0],pos[1]]\n",
    "\n",
    "        # crop around the area and add to training data\n",
    "        _, crop_spr = self.crop(cur_spr2,pos,crop_size)\n",
    "\n",
    "        return cur_spr2, og_val, crop_spr\n",
    "    \n",
    "    # calculate the hamming distance from 2 sprites\n",
    "    def calc_ham_dist(self, spr1, spr2):\n",
    "        return np.sum(spr1 != spr2)\n",
    "\n",
    "    # return the sprite that is closest in hamming distance from the goal set\n",
    "    def closest_spr(self, goal_set, noise_spr):\n",
    "        min_dist_i = np.argmin([self.calc_ham_dist(noise_spr, i) for i in goal_set])\n",
    "        return goal_set[min_dist_i]\n",
    "\n",
    "    \n",
    "    # make the training data for the path of destruction\n",
    "    def make_train_dat(self,goal_img_set,goal_enc_set,num_noise_spr=1000):\n",
    "        train_dat = []\n",
    "        with tqdm(total=num_noise_spr) as pbar:\n",
    "            for i in range(num_noise_spr):\n",
    "                # make a noise sprite and find the closest\n",
    "                # noise_spr = self.init_noise_sprite()\n",
    "                # targ_spr = self.closest_spr(goal_set,noise_spr).copy()\n",
    "\n",
    "                #go through each goal sprite (with associated annotation) and make a noise sprite for it\n",
    "                targ_spr = goal_img_set[i%len(goal_img_set)]\n",
    "                in_enc = goal_enc_set[i%len(goal_enc_set)]\n",
    "                noise_spr = self.init_noise_sprite()\n",
    "\n",
    "                # make a list of all the positions in the sprite and shuffle it\n",
    "                pos_set = []\n",
    "                for x in range(self.spr_shape[0]):\n",
    "                    for y in range(self.spr_shape[1]):\n",
    "                        pos_set.append((x,y))\n",
    "                random.shuffle(pos_set)\n",
    "\n",
    "                # iterate over the goal sprite and change until it matches the noise spr (destroy)\n",
    "                for p in pos_set:\n",
    "                    # destroy the noise sprite\n",
    "                    targ_spr, og_val, crop_spr = self.destroy(targ_spr,noise_spr,p,self.crop_size)\n",
    "                    train_dat.append([crop_spr,in_enc,og_val])\n",
    "                    # showSprPalette(targ_spr)\n",
    "                pbar.update(1)\n",
    "                    \n",
    "        return train_dat\n",
    "    \n",
    "    # make the path of destruction neural network model\n",
    "    # copy of architecture from the paper [Conv(128,3) -> Conv(128,3) -> MaxPool(2), Conv(256,3)]\n",
    "    # it doesn't work though... at least not with 5x5 crop size - need to ask Sam for exact implementation\n",
    "    def makePoDCNN(self):\n",
    "\n",
    "        # image input\n",
    "        in1 = Input(shape=(self.crop_size,self.crop_size,1))\n",
    "        in1 = Lambda(lambda x: K.cast(x, dtype='float32'))(in1)\n",
    "\n",
    "        # text encoding input\n",
    "        in2 = Input(shape=(self.ENC_SIZE,)),\n",
    "        in2r = K.reshape(in2,(-1,8,8,12))\n",
    "        in2f = Flatten()(in2r)\n",
    "        in2d = Dense(self.crop_size*self.crop_size)(in2f)\n",
    "        in2r2 = K.reshape(in2d,(-1,self.crop_size,self.crop_size,1))\n",
    "        # in2r2 = Conv1D(1, 3, activation='relu',padding='SAME')(in2r)\n",
    "\n",
    "        # combined and sent through the CNN\n",
    "        x = Concatenate(axis=-1)([in1,in2r2])\n",
    "        x = Conv2D(128, (3, 3), activation='relu',padding='SAME')(x)\n",
    "        x = MaxPooling2D((2, 2))(x)\n",
    "        x = Conv2D(128, (3, 3), activation='relu',padding='SAME')(x)\n",
    "        x = Conv2D(256, (3, 3), activation='relu',padding='SAME')(x)\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(128)(x)\n",
    "        out1 = Dense(self.channels,activation='softmax')(x)\n",
    "\n",
    "        self.pod_model = Model(inputs=[in1,in2], outputs=out1)\n",
    "        self.pod_model.summary()\n",
    "        self.opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "        self.pod_model.compile(optimizer=self.opt,metrics=['accuracy'],loss='sparse_categorical_crossentropy') #not one-hot encoded\n",
    "        \n",
    "        \n",
    "    # import a model from a path\n",
    "    def importModel(self,model_path):\n",
    "        self.pod_model = tf.keras.models.load_model(model_path)\n",
    "        self.pod_model.summary()\n",
    "        self.opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "        self.pod_model.compile(optimizer=self.opt,metrics=['accuracy'],loss='sparse_categorical_crossentropy')\n",
    "\n",
    "\n",
    "    # export the model to a path\n",
    "    def exportModel(self,model_path):\n",
    "        self.pod_model.save(model_path,save_format='h5')\n",
    "                            \n",
    "\n",
    "    # train the path of destruction model on the training data\n",
    "    def trainPoD(self,train_dat,EPOCHS=500,BATCH_SIZE=64,show_acc=False):\n",
    "        # train the model\n",
    "        X1 = np.array([i[0] for i in train_dat])\n",
    "        X2 = np.array([i[1] for i in train_dat])\n",
    "        Y = np.array([i[2] for i in train_dat])\n",
    "        X1 = np.expand_dims(X1,axis=-1)\n",
    "        # X2 = np.expand_dims(X2,axis=-1)\n",
    "        Y = np.expand_dims(Y,axis=-1)\n",
    "        print(X1.shape,X2.shape,Y.shape)\n",
    "\n",
    "        h = self.pod_model.fit([X1,X2],Y,epochs=EPOCHS,batch_size=BATCH_SIZE,shuffle=True)\n",
    "        if show_acc:\n",
    "            plt.plot(h.history['accuracy'])\n",
    "            plt.show()\n",
    "\n",
    "    # repair from a noise vector\n",
    "    def repair(self,text_in,init_spr=None,mod_iter='rand',eval_met='iter',num_iter=1000,animate=False):\n",
    "        #encode the text\n",
    "        text_enc = sentEmb(text_in)\n",
    "        text_enc = np.expand_dims(text_enc,axis=0)\n",
    "        print(text_enc.shape)\n",
    "\n",
    "        # make a noise sprite\n",
    "        if init_spr is None:\n",
    "            init_spr = self.init_noise_sprite()\n",
    "        cur_spr = np.copy(init_spr)\n",
    "\n",
    "        if animate:\n",
    "            anim_set = []\n",
    "            anim_set.append(cur_spr)\n",
    "\n",
    "        # iterate over the noise sprite and change until the iterations are done\n",
    "        pi = 0\n",
    "        if eval_met == \"iter\":\n",
    "            with tqdm(total=num_iter) as pbar:\n",
    "                for i in range(num_iter):\n",
    "                    #select a position (randomly or sequentially)\n",
    "                    if mod_iter == \"rand\":\n",
    "                        x = np.random.randint(self.spr_shape[0])\n",
    "                        y = np.random.randint(self.spr_shape[1])\n",
    "                    else:\n",
    "                        x = pi%self.spr_shape[0]\n",
    "                        y = pi//self.spr_shape[0]\n",
    "                        pi += 1\n",
    "                        if pi >= self.spr_shape[0]*self.spr_shape[1]:\n",
    "                            pi = 0\n",
    "\n",
    "                    pbar.set_description(\"@ ({},{})\".format(x,y))\n",
    "\n",
    "                    #crop the area\n",
    "                    _, crop_spr = self.crop(cur_spr,(x,y),self.crop_size)\n",
    "\n",
    "                    # preproc\n",
    "                    crop_spr = np.expand_dims(np.expand_dims(crop_spr,axis=-1),axis=0)  #make it a batch of 1\n",
    "                    \n",
    "                    \n",
    "                    # get the pixel change input from the model\n",
    "                    # pred_px = self.pod_model.predict(np.array([crop_spr,text_enc]),verbose=False)\n",
    "                    pred_px = self.pod_model.predict([crop_spr,text_enc],verbose=False)\n",
    "\n",
    "                    # change the tile\n",
    "                    cur_spr[x,y] = np.argmax(pred_px)\n",
    "\n",
    "                    anim_set.append(cur_spr.copy())\n",
    "\n",
    "                    pbar.update(1)\n",
    "\n",
    "        if animate:\n",
    "            return init_spr, cur_spr, anim_set\n",
    "        return init_spr, cur_spr\n",
    "\n",
    "if EXPERIMENT == \"text_enc\":\n",
    "    # Make the Path of Destruction  model with the text encoded input\n",
    "    tepod = TECPoD((8,8),channels=16,crop_size=GEN_CONF['WINDOW'])     \n",
    "    tepod.makePoDCNN() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64000\n"
     ]
    }
   ],
   "source": [
    "if EXPERIMENT == \"text_enc\":\n",
    "\n",
    "    if CONFIGS[\"text_enc\"][\"NEW_TRAIN_DAT\"]:\n",
    "        # make the training data\n",
    "        goal_spr_char = pico_char_dat.copy()\n",
    "        goal_enc_char = pico_char_labels_emb.copy()\n",
    "        train_dat = tepod.make_train_dat(goal_spr_char,goal_enc_char,num_noise_spr=(len(goal_spr_char)*10))\n",
    "\n",
    "        # ninja turtle set\n",
    "        # turtle_spr_char = pico_char_dat[30:34]\n",
    "        # showMultiSprPalette(turtle_spr_char)\n",
    "        # train_dat = npod.make_train_dat(turtle_spr_char,num_noise_spr=(len(turtle_spr_char)*100))\n",
    "\n",
    "        print(len(train_dat))\n",
    "        print(train_dat[0][0].shape,train_dat[0][1].shape,train_dat[0][2])\n",
    "\n",
    "        # export as .npy file\n",
    "        np.save(CONFIGS[\"text_enc\"][\"TRAIN_DAT_PATH\"],train_dat)\n",
    "    else:\n",
    "        # import the training data\n",
    "        train_dat = np.load(CONFIGS['text_enc'][\"TRAIN_DAT_PATH\"],allow_pickle=True)\n",
    "        print(len(train_dat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 768)]        0           []                               \n",
      "                                                                                                  \n",
      " tf.reshape (TFOpLambda)        (None, 8, 8, 12)     0           ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 768)          0           ['tf.reshape[0][0]']             \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           49216       ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 8, 8, 1)]    0           []                               \n",
      "                                                                                                  \n",
      " tf.reshape_1 (TFOpLambda)      (None, 8, 8, 1)      0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 8, 8, 2)      0           ['input_3[0][0]',                \n",
      "                                                                  'tf.reshape_1[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 8, 8, 128)    2432        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 4, 4, 128)    0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 4, 4, 128)    147584      ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 4, 4, 256)    295168      ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 4096)         0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 128)          524416      ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 16)           2064        ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,020,880\n",
      "Trainable params: 1,020,880\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# train or import the model\n",
    "if EXPERIMENT == \"text_enc\":\n",
    "    if CONFIGS[\"text_enc\"][\"TRAIN_TEPOD\"]:\n",
    "        tepod.trainPoD(train_dat,EPOCHS=GEN_CONF['EPOCHS'],BATCH_SIZE=GEN_CONF['BATCH_SIZE'],show_acc=True)\n",
    "        tepod.exportModel(CONFIGS[\"text_enc\"][\"TEPOD_MODEL\"])\n",
    "    else:\n",
    "        tepod.importModel(CONFIGS[\"text_enc\"][\"TEPOD_MODEL\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "@ (7,7): 100%|██████████| 192/192 [00:09<00:00, 19.99it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "saving GIF 193...: 100%|██████████| 193/193 [00:09<00:00, 19.57it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "TEXT = \"a man with yellow pants\" \n",
    "# show the output from repairing a noise sprite\n",
    "if EXPERIMENT == \"text_enc\":\n",
    "    init_spr, cur_spr, anims = tepod.repair(TEXT,mod_iter='seq',num_iter=192,animate=True)\n",
    "    showMultiSprPalette([init_spr,cur_spr],textArr=['init','final'])\n",
    "\n",
    "    # save the animation\n",
    "    animatePal(anims,f\"../prelim_output/pod_anim/tepod_{GEN_CONF['EPOCHS']}e_w{GEN_CONF['WINDOW']}-yellow_pants_man.gif\",fps=32,textArr=[f\"Iter: {i}\" for i in range(len(anims))])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5h/0zjb1m296y1gvhtd9q40w4yc0000gn/T/ipykernel_6927/3430132365.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x = np.array([init_spr,sentEmb(TEXT)])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/milk/Desktop/GIL_Lab/BMO/BMO_chatbot_prototype/Experimental_Notebooks/sprite_pod.ipynb Cell 25\u001b[0m in \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/milk/Desktop/GIL_Lab/BMO/BMO_chatbot_prototype/Experimental_Notebooks/sprite_pod.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([init_spr,sentEmb(TEXT)])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/milk/Desktop/GIL_Lab/BMO/BMO_chatbot_prototype/Experimental_Notebooks/sprite_pod.ipynb#X40sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m x \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mcast(x,tf\u001b[39m.\u001b[39;49mfloat32)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/milk/Desktop/GIL_Lab/BMO/BMO_chatbot_prototype/Experimental_Notebooks/sprite_pod.ipynb#X40sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# print(x)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/milk/Desktop/GIL_Lab/BMO/BMO_chatbot_prototype/Experimental_Notebooks/sprite_pod.ipynb#X40sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m tepod\u001b[39m.\u001b[39mpod_model\u001b[39m.\u001b[39mpredict(x)\n",
      "File \u001b[0;32m~/Desktop/GIL_Lab/BMO/BMO_chatbot_prototype/bmo-venv/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Desktop/GIL_Lab/BMO/BMO_chatbot_prototype/bmo-venv/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[1;32m    101\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray)."
     ]
    }
   ],
   "source": [
    "x = np.array([init_spr,sentEmb(TEXT)])\n",
    "x = tf.cast(x,tf.float32)\n",
    "# print(x)\n",
    "tepod.pod_model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 4]\n",
      "   [ 8]\n",
      "   [ 2]\n",
      "   [ 1]\n",
      "   [10]\n",
      "   [12]\n",
      "   [ 8]\n",
      "   [12]]\n",
      "\n",
      "  [[11]\n",
      "   [ 5]\n",
      "   [ 9]\n",
      "   [ 5]\n",
      "   [12]\n",
      "   [ 8]\n",
      "   [ 5]\n",
      "   [ 0]]\n",
      "\n",
      "  [[13]\n",
      "   [ 7]\n",
      "   [15]\n",
      "   [ 2]\n",
      "   [ 6]\n",
      "   [14]\n",
      "   [11]\n",
      "   [ 0]]\n",
      "\n",
      "  [[ 2]\n",
      "   [ 9]\n",
      "   [15]\n",
      "   [ 8]\n",
      "   [ 0]\n",
      "   [13]\n",
      "   [10]\n",
      "   [10]]\n",
      "\n",
      "  [[ 0]\n",
      "   [11]\n",
      "   [ 9]\n",
      "   [ 7]\n",
      "   [14]\n",
      "   [ 9]\n",
      "   [10]\n",
      "   [ 2]]\n",
      "\n",
      "  [[10]\n",
      "   [12]\n",
      "   [12]\n",
      "   [ 0]\n",
      "   [13]\n",
      "   [12]\n",
      "   [ 6]\n",
      "   [ 8]]\n",
      "\n",
      "  [[ 6]\n",
      "   [13]\n",
      "   [13]\n",
      "   [15]\n",
      "   [ 1]\n",
      "   [12]\n",
      "   [ 0]\n",
      "   [ 2]]\n",
      "\n",
      "  [[ 4]\n",
      "   [ 3]\n",
      "   [10]\n",
      "   [ 7]\n",
      "   [ 7]\n",
      "   [ 9]\n",
      "   [ 0]\n",
      "   [ 7]]]]\n",
      "[[-3.06603551e-01 -5.21848910e-02 -2.39456630e+00  3.94117497e-02\n",
      "   1.16880012e+00 -4.50373173e-01 -1.70885360e+00 -1.10859394e-01\n",
      "  -6.38351381e-01 -3.49628270e-01 -5.46912849e-01  1.46670485e+00\n",
      "   3.54589045e-01 -1.33053362e-01  4.06604171e-01  1.05487527e-02\n",
      "  -6.32065475e-01 -8.06774423e-02 -9.18105841e-02  7.95490205e-01\n",
      "  -7.04753757e-01 -3.07381243e-01 -7.42180347e-01 -6.82968557e-01\n",
      "   5.07640302e-01 -5.25882952e-02 -1.67007625e-01  1.05655760e-01\n",
      "  -7.49366581e-02  4.68095928e-01  3.26381564e-01 -1.62120864e-01\n",
      "   4.97325420e-01  2.76810154e-02 -3.71236056e-01  5.09760618e-01\n",
      "  -9.25639451e-01 -1.38758659e+00 -1.03464790e-01 -4.73190472e-02\n",
      "   1.35666525e+00  3.72883938e-02 -2.64978051e-01  4.34652299e-01\n",
      "  -4.55464959e-01  2.32954532e-01 -7.45576695e-02  1.39276057e-01\n",
      "  -2.06288069e-01  7.78115869e-01 -3.55414391e-01 -3.92821968e-01\n",
      "  -1.01391768e+00  3.14479247e-02 -1.26561895e-01 -2.44254380e-01\n",
      "   2.37725466e-01  5.66338301e-01  1.61292583e-01 -1.02083719e+00\n",
      "  -8.98912728e-01 -9.51929927e-01 -6.42466486e-01  5.57866454e-01\n",
      "  -1.72015592e-01  6.03655517e-01 -1.11415781e-01 -3.51696879e-01\n",
      "  -4.99289364e-01  2.62129366e-01  8.92163992e-01 -5.38260877e-01\n",
      "  -4.35705781e-01  3.44653577e-01  1.26515046e-01 -1.59510300e-01\n",
      "  -6.35967135e-01 -1.69795007e-01 -3.41017753e-01  3.07245433e-01\n",
      "   4.60440636e-01 -2.15574726e-02  2.74114341e-01 -5.93662500e-01\n",
      "   1.60330892e-01 -3.30663323e-01 -2.97310412e-01  1.79488397e+00\n",
      "   1.80114508e-01  3.47574741e-01  1.21236086e-01 -3.49797577e-01\n",
      "   8.23775291e-01  1.09069371e+00  2.27318764e-01 -8.65577832e-02\n",
      "  -2.09574863e-01 -8.45249221e-02  1.32405996e-01 -5.03645897e-01\n",
      "  -4.03685480e-01  7.94176161e-01 -3.97082597e-01  7.65213966e-01\n",
      "  -7.15933442e-01 -5.36868215e-01 -1.31822184e-01 -4.36350703e-01\n",
      "  -7.92421043e-01  6.40808105e-01  2.92956710e-01  6.40948296e-01\n",
      "  -3.78324166e-02 -1.40631646e-01 -2.28625774e-01 -1.99467346e-01\n",
      "   4.53742385e-01  1.01627350e+00 -5.64135969e-01  3.59330028e-01\n",
      "  -5.06229281e-01  3.07593495e-01 -1.25283487e-02  1.23054922e+00\n",
      "   1.82380927e+00 -1.40497582e-02 -1.95147231e-01  4.99361098e-01\n",
      "  -1.30880460e-01 -1.53457746e-01 -2.80896068e-01  6.40078485e-01\n",
      "   5.23147821e-01 -2.64903218e-01 -2.31162652e-01  8.40890408e-02\n",
      "  -2.43250579e-01 -6.48826599e-01  1.15467377e-01 -5.60211182e-01\n",
      "  -1.44385636e+00  4.89229739e-01  5.63765466e-01  3.83277416e-01\n",
      "   1.27556935e-01 -3.23906213e-01  1.02956700e+00  4.14750010e-01\n",
      "  -1.30745918e-01 -3.46692204e-01 -1.84615612e-01 -5.64591587e-01\n",
      "  -1.65116623e-01  3.44677001e-01 -1.57713100e-01  5.40845990e-01\n",
      "  -9.77605700e-01 -1.96786195e-01  1.72028214e-01  3.51210594e-01\n",
      "   1.54686511e-01 -1.55424520e-01 -1.95193782e-01  3.83192062e-01\n",
      "   2.69975632e-01 -4.33883339e-01 -8.91732931e-01  7.19408512e-01\n",
      "  -3.03037494e-01 -2.61400878e-01 -3.21195126e-01 -4.65399414e-01\n",
      "   1.01496056e-01  1.27174997e+00  4.58829045e-01 -4.60414767e-01\n",
      "   4.55225617e-01  1.74554443e+00  8.59400332e-01 -1.10013556e+00\n",
      "   1.11549348e-01 -4.64886308e-01  1.43416405e+00 -8.11078548e-01\n",
      "  -2.76318073e-01  1.22430868e-01 -1.75024822e-01 -3.46797943e-01\n",
      "   3.89702678e-01  4.96245503e-01 -9.13030326e-01 -3.95933747e-01\n",
      "  -4.49714810e-02 -2.64396280e-01  2.03604937e-01  1.88733160e-01\n",
      "  -1.35618114e+00  4.69081253e-02 -8.45517635e-01 -4.55902517e-01\n",
      "   2.49839410e-01  3.37234199e-01  5.49160004e-01  2.20040634e-01\n",
      "  -3.35855126e-01  2.78650731e-01  3.46436620e-01 -5.29426076e-02\n",
      "  -2.67629564e-01  1.17235474e-01 -1.23698759e+00  1.63942254e+00\n",
      "   1.51050210e-01  2.17931822e-01  6.35469615e-01 -3.61805111e-01\n",
      "   7.21139252e-01 -6.27785563e-01  8.19962144e-01  1.79021537e-01\n",
      "   6.21594608e-01  3.98369610e-01  1.22769222e-01  7.82596231e-01\n",
      "  -5.77031896e-02  8.34698141e-01  5.72272658e-01  1.63887322e-01\n",
      "   9.88128364e-01  1.41884696e+00  3.18815351e-01  4.50291693e-01\n",
      "  -5.29012322e-01 -9.61502939e-02  8.76944065e-01  3.88198793e-01\n",
      "   2.22873658e-01 -7.67943919e-01  1.16211057e+00  5.27170300e-01\n",
      "   6.52779341e-01 -7.02242792e-01 -2.43131265e-01  2.25868016e-01\n",
      "  -1.44373253e-01 -2.31100217e-01  3.10792089e-01  9.41341184e-03\n",
      "  -4.96268690e-01 -2.72230655e-01 -8.54685724e-01 -7.81092048e-01\n",
      "   8.92862082e-01 -7.35149920e-01  7.49250710e-01 -6.89660549e-01\n",
      "   3.81327778e-01  4.43427444e-01 -6.55408502e-01 -7.84671724e-01\n",
      "  -6.12332642e-01 -6.47790313e-01 -1.00724959e+00  2.94794500e-01\n",
      "  -1.05206645e+00 -4.07520831e-01 -4.45906110e-02  4.89510804e-01\n",
      "   2.24589616e-01 -5.28335333e-01 -4.58334982e-01  6.31022871e-01\n",
      "  -1.98109280e-02 -5.61444819e-01 -1.05592251e-01 -1.14363611e+00\n",
      "   4.41252649e-01 -8.47797990e-02 -5.53913951e-01 -2.32442096e-02\n",
      "   6.15239561e-01  5.89980841e-01 -1.42178833e+00  4.32212472e-01\n",
      "   2.06506550e-01  7.63233781e-01 -3.53783406e-02 -1.68657017e+00\n",
      "  -3.76301169e-01  5.34887731e-01  3.25826079e-01 -6.09516323e-01\n",
      "  -7.53856897e-01 -2.48795301e-01 -5.06888747e-01 -5.88027947e-03\n",
      "  -6.80920631e-02 -1.56429350e+00 -7.12011158e-01  1.84988961e-01\n",
      "  -2.43326854e-02 -4.06013072e-01 -1.60780773e-01  1.96360648e-01\n",
      "   4.87491786e-02  5.46825230e-01  7.18517542e-01  6.13654852e-01\n",
      "  -4.96140480e-01  9.27656710e-01 -7.69433677e-01  1.37824744e-01\n",
      "  -1.06946480e+00  4.01565611e-01 -5.24042845e-01  9.09276530e-02\n",
      "   5.03545940e-01 -2.92542040e-01 -1.14325261e+00 -7.91400015e-01\n",
      "   5.01063347e-01  4.69085842e-01 -4.08394992e-01  2.04104707e-01\n",
      "   7.58540213e-01  3.22017401e-01 -6.98935628e-01  2.21651569e-01\n",
      "  -2.09454611e-01 -3.44042361e-01  7.53447711e-01 -7.02043653e-01\n",
      "  -2.43620574e-01  1.85303938e+00  7.81767964e-01 -3.50176364e-01\n",
      "  -4.94156629e-01 -9.21974897e-01 -3.33021104e-01 -2.24932939e-01\n",
      "   2.55967826e-01 -4.31673855e-01 -2.16629282e-02  4.31198090e-01\n",
      "  -4.74527150e-01 -1.57356843e-01 -7.58237720e-01  2.91225940e-01\n",
      "  -2.03008905e-01 -7.65470803e-01 -1.02993786e-01 -1.01561260e+00\n",
      "  -5.36357641e-01  1.48987591e-01  6.05169296e-01 -1.22024447e-01\n",
      "   2.77604550e-01  5.12740970e-01 -6.91070318e-01  4.16656315e-01\n",
      "   3.07577085e-02  3.94090593e-01  3.97780716e-01 -2.07390517e-01\n",
      "   1.80551276e-01  4.41682130e-01  4.09765005e-01 -8.16634476e-01\n",
      "   4.74168658e-01 -9.42921937e-01 -2.49967173e-01  3.63840237e-02\n",
      "   4.50176984e-01  2.68883765e-01 -2.29899108e-01 -5.16638935e-01\n",
      "  -3.07676703e-01 -1.14633393e+00  1.58857524e-01  7.77980685e-01\n",
      "  -2.14149103e-01 -5.83827905e-02  1.92917943e-01  9.43130478e-02\n",
      "  -5.22431433e-01 -1.17523700e-01  4.73288536e-01 -4.59228098e-01\n",
      "  -1.26079571e+00 -4.31404173e-01 -3.69504452e-01  3.51311356e-01\n",
      "  -4.31996375e-01 -3.76831591e-01 -8.28438476e-02  3.21731091e-01\n",
      "   5.00760198e-01 -1.88324973e-02  5.57163715e-01  9.33810472e-01\n",
      "   2.94195056e-01 -1.58138484e-01  9.55120087e-01 -8.05739224e-01\n",
      "   9.17419553e-01 -3.28150630e-01  8.43159437e-01  5.33323698e-02\n",
      "   8.37630779e-02  1.03979493e-02  2.58470982e-01 -4.60557878e-01\n",
      "  -7.04036593e-01 -4.88439769e-01 -1.34749249e-01 -1.56152755e-01\n",
      "  -5.10381281e-01 -7.00457335e-01  6.00451648e-01  1.75562277e-02\n",
      "  -1.96085721e-01 -6.83213174e-01  6.09285951e-01  9.04165566e-01\n",
      "   2.57176280e-01  3.43640447e-01  1.91724226e-01 -5.42791247e-01\n",
      "   3.75448972e-01  1.30678034e+00 -6.73182309e-02 -8.25714469e-01\n",
      "  -1.47617787e-01  5.27271986e-01  5.31885803e-01  5.88106923e-02\n",
      "  -9.98635739e-02 -1.66073382e-01  4.94934469e-02  6.26470596e-02\n",
      "   5.46990335e-01  1.98632300e-01  1.59516788e+00  5.27635396e-01\n",
      "  -8.18006217e-01 -5.83438203e-02 -5.10565042e-01  8.29759479e-01\n",
      "   2.42333144e-01 -1.23500161e-01  7.26062953e-01  3.64615381e-01\n",
      "  -3.41475010e-01  7.21650794e-02  7.86214769e-01 -6.97680712e-01\n",
      "   2.85410374e-01 -4.47449274e-02 -1.09457441e-01 -3.53368849e-01\n",
      "  -1.23974746e-02  7.89662749e-02 -9.80116308e-01  4.39586520e-01\n",
      "  -3.33056808e-01  3.56000997e-02 -5.93686521e-01  5.52291155e-01\n",
      "   1.16660446e-01  4.57734227e-01  2.97679484e-01 -2.62036800e-01\n",
      "   9.66813624e-01  1.09800494e+00  7.78191030e-01  4.26369570e-02\n",
      "  -9.27313268e-01  4.60762769e-01 -5.17408490e-01  4.55320418e-01\n",
      "  -9.10436392e-01  1.62899435e-01  7.27333367e-01 -4.36352193e-01\n",
      "   1.07925728e-01 -8.18472207e-02  5.82619965e-01  6.65401638e-01\n",
      "   3.86814475e-01 -7.41557181e-01 -7.78149394e-03 -4.57943112e-01\n",
      "   1.51372654e-02 -1.37154841e+00 -3.04910988e-01  1.75864172e+00\n",
      "  -5.42503953e-01 -7.53771245e-01  8.94368172e-01 -6.07584238e-01\n",
      "  -3.99156690e-01  6.34610832e-01 -9.25189555e-01 -1.20705426e+00\n",
      "   1.01553775e-01 -5.15144646e-01  5.76806307e-01 -8.02060008e-01\n",
      "  -3.28838974e-01 -6.70308709e-01 -4.99564379e-01 -6.56003118e-01\n",
      "  -4.79142070e-01 -6.50953770e-01  8.30212235e-02  3.35333467e-01\n",
      "  -7.35421836e-01  9.94178429e-02 -2.05242053e-01 -7.78560400e-01\n",
      "   1.46010864e+00 -5.78466654e-01 -7.29718685e-01 -3.46446186e-01\n",
      "  -2.73294300e-02 -2.89927095e-01  2.41467103e-01 -7.02749431e-01\n",
      "  -2.24902973e-01 -5.39930880e-01  4.81768250e-01  3.69282290e-02\n",
      "  -3.07736322e-02  5.44439435e-01 -5.53850412e-01  8.46765280e-01\n",
      "  -6.65232316e-02 -5.20003557e-01  1.20741874e-02  2.91409135e-01\n",
      "  -1.02782929e+00  5.72137594e-01 -1.67338718e-02 -4.92695332e-01\n",
      "   5.46626806e-01  1.73387155e-01  4.41770181e-02 -5.08693516e-01\n",
      "   1.25000572e+00  1.32592618e-01 -3.08747739e-01  9.13005590e-01\n",
      "   3.07552684e-02  6.60491765e-01 -7.71019697e-01  4.63353544e-02\n",
      "  -8.03986788e-01  3.85973096e-01 -1.85493290e-01  2.12069526e-01\n",
      "   6.08555496e-01 -3.27615380e-01 -4.26471084e-01 -1.67934209e-01\n",
      "  -8.90549958e-01 -2.14070734e-03 -7.45359004e-01 -2.34450877e-01\n",
      "  -4.46008831e-01  6.38229966e-01 -3.26459706e-02 -1.03969431e+00\n",
      "   1.59885228e+00  7.77127683e-01 -6.64707065e-01 -3.12988311e-01\n",
      "   8.69342983e-01 -6.04421377e-01 -2.56128967e-01  2.37117633e-01\n",
      "   4.75545883e-01  4.97155190e-01 -5.68788312e-02 -4.68209893e-01\n",
      "  -2.78363794e-01 -3.70582998e-01  4.54183161e-01 -8.96969378e-01\n",
      "  -3.01117718e-01  6.43190384e-01 -2.95849470e-03  4.29480761e-01\n",
      "   2.03508586e-01  1.13145181e-03  2.73338091e-02  4.32504296e-01\n",
      "  -3.10041338e-01  1.27593720e+00 -1.25908434e-01 -8.54926169e-01\n",
      "   6.72883749e-01 -1.30198240e+00  2.50471622e-01  8.67068395e-02\n",
      "   6.38667867e-02  2.31060207e-01 -9.74661410e-01 -1.34910494e-01\n",
      "  -7.41854787e-01  7.82092333e-01  3.32339853e-01 -3.38778123e-02\n",
      "   5.70086181e-01 -2.04277098e-01 -5.67221105e-01 -5.63357994e-02\n",
      "   7.94067204e-01  3.40260476e-01  3.30876231e-01  1.46928757e-01\n",
      "  -6.43768549e-01  9.81198430e-01 -1.08088148e+00 -1.50439233e-01\n",
      "   7.30683565e-01 -1.10471642e+00  8.15190077e-01  4.13918942e-02\n",
      "   1.72476590e-01 -1.40023530e-01 -4.71149571e-03 -1.44168302e-01\n",
      "  -7.51231313e-01  2.80197233e-01 -7.24017441e-01  1.07953846e+00\n",
      "   5.23999035e-01  4.11844939e-01 -9.60593522e-02 -3.34454060e-01\n",
      "  -5.68464920e-02 -1.63717782e+00 -1.29911852e+00 -3.36297005e-02\n",
      "   1.10428996e-01  1.83118239e-01  1.56353310e-01  1.04888535e+00\n",
      "   8.20271730e-01 -2.27992058e-01 -4.17426407e-01  2.13012666e-01\n",
      "   1.44614011e-01  3.49907666e-01 -2.99432009e-01 -9.55410004e-01\n",
      "  -1.71228368e-02 -1.80420965e-01  4.38244939e-01 -8.26004818e-02\n",
      "   2.60583818e-01 -3.16854388e-01 -4.11276966e-01 -4.00053769e-01\n",
      "  -6.42958701e-01  6.17521524e-01  5.32729447e-01  2.33401716e-01\n",
      "   6.10308051e-02  8.50513875e-01 -4.08097863e-01  3.01753152e-02\n",
      "   7.96748042e-01  5.40416986e-02  2.64243931e-01  9.33272481e-01\n",
      "   5.44784307e-01  5.55105865e-01 -1.00040138e+00 -2.81469822e-02\n",
      "  -3.42896014e-01 -4.59116250e-01 -6.98872864e-01 -4.99434888e-01\n",
      "  -1.39716744e-01 -2.92105585e-01 -1.09028304e+00  9.75556374e-01\n",
      "  -5.22835255e-01 -9.32738960e-01 -4.19685125e-01  1.05519257e-01\n",
      "  -1.27817884e-01 -4.95050639e-01  1.49328321e-01  5.20224571e-01\n",
      "   6.90122485e-01  1.55776903e-01  1.03574023e-01  4.40687984e-01\n",
      "  -2.61875242e-01  2.79293388e-01 -1.56918705e+00  5.54508194e-02\n",
      "   2.70968586e-01  7.86706746e-01  8.86962786e-02 -8.40343237e-01\n",
      "   9.84284729e-02  6.24729156e-01  8.35602105e-01  7.42484033e-01\n",
      "  -5.38540125e-01  2.32359722e-01 -2.96090424e-01 -1.66746285e-02\n",
      "  -4.34497982e-01 -4.30175900e-01  4.74490047e-01  4.34513837e-01\n",
      "  -5.83172858e-01  6.91213846e-01 -8.41027498e-01 -4.71214414e-01\n",
      "  -1.29028141e-01  1.13644767e+00 -4.79097664e-03  8.96395802e-01\n",
      "  -8.63776207e-01 -3.86504322e-01 -9.63996351e-01 -4.84534681e-01\n",
      "  -2.19357237e-01 -4.31274325e-01  7.22678840e-01 -2.71736607e-02\n",
      "  -2.16107890e-01 -2.12970585e-01 -8.17135811e-01  9.50392425e-01\n",
      "  -6.71070218e-01 -9.83381391e-01 -9.06756148e-02 -6.46693468e-01\n",
      "  -2.49011114e-01 -3.57286483e-01 -3.58741850e-01  8.95646811e-01\n",
      "  -5.51873863e-01  8.97099197e-01  6.47804976e-01  2.28604242e-01\n",
      "  -2.63212711e-01  1.75625876e-01 -3.03328305e-01  5.84731817e-01\n",
      "   5.88286109e-02 -4.67815995e-01  2.47266605e-01  4.71687734e-01\n",
      "   2.56609589e-01  1.09983456e+00 -1.23905741e-01 -1.18423402e+00\n",
      "  -3.31629604e-01  1.08705425e+00  5.93292534e-01 -2.80489266e-01]]\n",
      "(1, 8, 8, 1) (1, 768)\n",
      "1/1 [==============================] - 0s 94ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[8.0725272e-07, 3.8834944e-16, 3.8286574e-12, 2.4268692e-04,\n",
       "        9.8936123e-01, 1.6406036e-08, 3.6607117e-03, 6.7233262e-03,\n",
       "        1.5686749e-13, 1.0776702e-05, 1.2969760e-11, 4.1335880e-12,\n",
       "        9.8043373e-10, 1.5044901e-18, 3.5768513e-16, 4.8433805e-07]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# img = np.expand_dims(init_spr,axis=-1)\n",
    "_, img = tepod.crop(init_spr,(4,4),8)\n",
    "img = np.expand_dims(np.expand_dims(img,axis=-1),axis=0)\n",
    "e = np.expand_dims(sentEmb(TEXT),axis=0)\n",
    "print(img)\n",
    "print(e)\n",
    "\n",
    "print(img.shape,e.shape)\n",
    "\n",
    "tepod.pod_model.predict([img,e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 8) (768,) ()\n"
     ]
    }
   ],
   "source": [
    "d = train_dat[0]\n",
    "print(d[0].shape,d[1].shape,d[2].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bmo-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f94925f3cff0b0ac90dd538919dcd2d4d02467fa3fdafdf86812e92b857d43c1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
