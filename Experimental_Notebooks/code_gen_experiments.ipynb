{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE SNIPPET GENERATION EXPERIMENTS \n",
    "Generates code (pseudocode, function snippets from comment prompt generation) using genetic programming and transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/milk/Desktop/GIL_Lab/BMO/BMO_chatbot_prototype/bmo-venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 1: CodeGenT5\n",
    "Note: Can generate from a snippet of Python code. Modifying the max_time, max_new_tokens, and input string (comment with snippet of code) affects how accurate it returns the value. Needs a starter function with parameters in order to actually write the code (follow comment with a \"\\ndef x(\"). Doesn't improve with beam search. Low temperature is best.\n",
    "\n",
    "\\> Input: `\"# write a function that multiplies all numbers in a list by a random number\\ndef x(\"`\n",
    "\n",
    "\\> Options:\n",
    "```\n",
    "options = {\n",
    "    \"max_new_tokens\": 100,\n",
    "    \"temperature\": 0.2,    #lower is better for more efficient code\n",
    "    # \"repetition_penalty\": 0.01,\n",
    "    \"do_sample\": True,\n",
    "    \"max_time\": 10,   #maximum time allotted to generate\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "\\> Output: \n",
    "\n",
    "```# write a function that multiplies all numbers in a list by a random number\n",
    "def x(n):\n",
    "    return n*random.randint(1,10)\n",
    "\n",
    "# write a function that takes a list of numbers and returns the largest number\n",
    "def largest(n):\n",
    "    return max(n)\n",
    "\n",
    "# write a function that```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    2,  3551,   257,  2163,   326, 15082,   444,   477,  3146,   287,\n",
      "          257,  1351,   416,   257,  4738,  1271,   198,  4299,  2124,     7,\n",
      "           77,  2599,   198, 50284,  7783,   299,     9, 25120,    13, 25192,\n",
      "          600,     7,    16,    11,   940,     8,   198,   198,     2,  3551,\n",
      "          257,  2163,   326,  2753,   257,  1351,   286,  3146,   290,  5860,\n",
      "          262,  2811,   286,   883,  3146,   198,  4299,  2811,     7,    77,\n",
      "         2599,   198, 50284,  7783,  2160,     7,    77, 20679, 11925,     7])\n",
      "# write a function that multiplies all numbers in a list by a random number\n",
      "def x(n):\n",
      "    return n*random.randint(1,10)\n",
      "\n",
      "# write a function that takes a list of numbers and returns the average of those numbers\n",
      "def average(n):\n",
      "    return sum(n)/len(\n",
      "=========================================\n",
      "tensor([    2,  3551,   257,  2163,   326, 15082,   444,   477,  3146,   287,\n",
      "          257,  1351,   416,   257,  4738,  1271,   198,  4299,  2124,     7,\n",
      "           77,  2599,   198, 50284,  7783,   299,  1635,  4738,    13, 25192,\n",
      "          600,     7,    16,    11,   940,     8,   198,   198,     2,  3551,\n",
      "          257,  2163,   326,  2753,   257,  1351,   286,  3146,   290,  5860,\n",
      "          262,  4387,  1271,   198,  4299,  4387,     7,    77,  2599,   198,\n",
      "        50284,  7783,  3509,     7,    77,     8,   198,   198,     2,  3551])\n",
      "# write a function that multiplies all numbers in a list by a random number\n",
      "def x(n):\n",
      "    return n * random.randint(1,10)\n",
      "\n",
      "# write a function that takes a list of numbers and returns the largest number\n",
      "def largest(n):\n",
      "    return max(n)\n",
      "\n",
      "# write\n",
      "=========================================\n",
      "tensor([    2,  3551,   257,  2163,   326, 15082,   444,   477,  3146,   287,\n",
      "          257,  1351,   416,   257,  4738,  1271,   198,  4299,  2124,     7,\n",
      "           77, 17024,  2599,   198, 50284,     2,  3551,   534,  2438,   994,\n",
      "          198, 50284,  7783,  3146,    58,    15,    60,  1635,  3146,    58,\n",
      "           16,    60,   198,   198,     2,  3551,   257,  2163,   326,  2753,\n",
      "          257,  1351,   286,  3146,   290,  5860,   262,  2160,   286,   262,\n",
      "         3146,   198,  4299,  2160,     7,    77, 17024,  2599,   198, 50284])\n",
      "# write a function that multiplies all numbers in a list by a random number\n",
      "def x(numbers):\n",
      "    # write your code here\n",
      "    return numbers[0] * numbers[1]\n",
      "\n",
      "# write a function that takes a list of numbers and returns the sum of the numbers\n",
      "def sum(numbers):\n",
      "    \n",
      "=========================================\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"Salesforce/codegen-350M-mono\"\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "text = \"# write a function that multiplies all numbers in a list by a random number\\ndef x(\"\n",
    "\n",
    "options = {\n",
    "    \"max_new_tokens\": 50,\n",
    "    \"temperature\": 0.2,    #lower is better for more efficient code\n",
    "    # \"repetition_penalty\": 0.01,\n",
    "    \"do_sample\": True,\n",
    "    \"max_time\": 10,   #maximum time allotted to generate\n",
    "    \"num_return_sequences\": 3,\n",
    "}\n",
    "completion = model.generate(**tokenizer(text, return_tensors=\"pt\"), **options)\n",
    "\n",
    "for c in completion:\n",
    "    print(c)\n",
    "    print(tokenizer.decode(c))\n",
    "    print(\"=========================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50256\n",
      "\n",
      "\n",
      "[2]\n",
      "hehe\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(198))\n",
    "print(tokenizer.encode(\"#\"))\n",
    "print(\"hehe\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 2: CodeGenT5 with cutoff\n",
    "Note: Gets best function definitions ([198, 198] token sequence in the encoding list.) Still needs a specification of the function parameters -> (comment + 'def x():\\n' with num parameters inside.) Phrasing is very important for communication to the transformer ('return random numbers that are multiples of 2' is harder than 'return a number that is a multiple of 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# return index if the a value from the set appears twice in a row in a tensor\n",
    "def hasDouble(x,vs=[198]):\n",
    "    # get all indexes where the value is v\n",
    "    idx = []\n",
    "    for v in vs:\n",
    "        idxi = (x == v).nonzero().flatten()\n",
    "        if len(idxi) > 0:\n",
    "            idx += idxi.tolist()\n",
    "    if len(idx) == 0:\n",
    "        return -1\n",
    "\n",
    "    #sort the indexes\n",
    "    idx.sort()\n",
    "    \n",
    "    # check if the next index is the same\n",
    "    for i in range(len(idx)-1):\n",
    "        if idx[i] == idx[i+1]-1:\n",
    "            return idx[i]\n",
    "    return -1\n",
    "\n",
    "\n",
    "a = torch.Tensor([420,69,69,13,7,628,198,2,1,198,198,0,21])\n",
    "print(hasDouble(a,[198,628]))  #should return 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# write a function that returns a random number that is a multiple of 2\n",
      "def x():\n",
      "    num = randint(1, 10)\n",
      "    if num % 2 == 0:\n",
      "        return num\n",
      "    else:\n",
      "        return 0\n",
      "=========================================\n",
      "# write a function that returns a random number that is a multiple of 2\n",
      "def x():\n",
      "    x = random.randint(0,100)\n",
      "    if x % 2 == 0:\n",
      "        return x\n",
      "    else:\n",
      "        return x + 1\n",
      "=========================================\n",
      "# write a function that returns a random number that is a multiple of 2\n",
      "def x():\n",
      "    # write your code here\n",
      "    return random.randint(0, 99)\n",
      "=========================================\n"
     ]
    }
   ],
   "source": [
    "# try again - cutting off after double 198 tokens\n",
    "txt = \"# write a function that returns a random number that is a multiple of 2\\ndef x():\\n\"\n",
    "options = {\n",
    "    \"max_new_tokens\": 50,\n",
    "    \"temperature\": 0.55,    #lower is better for more efficient code\n",
    "    \"repetition_penalty\": 0.99,\n",
    "    \"do_sample\": True,\n",
    "    \"max_time\": 10,   #maximum time allotted to generate\n",
    "    \"num_return_sequences\": 3,\n",
    "}\n",
    "completion = model.generate(**tokenizer(txt, return_tensors=\"pt\"), **options)\n",
    "\n",
    "for c in completion:\n",
    "    early_stop = hasDouble(c,[198,628])\n",
    "    # early_stop = hasDouble(c,[198])\n",
    "    print(tokenizer.decode(c[:early_stop]))\n",
    "    print(\"=========================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    2,  3551,   257,  2163,   326,  5860,  4738,  3146,   326,   389,\n",
      "         5021,  2374,   286,   362,   198,  4299,  2124, 33529,   198, 50284,\n",
      "           87,   796,  4738,    13, 25192,   600,     7,    16,    11,   838,\n",
      "            8,   198, 50284,   361,  2124,  4064,   362,  6624,   657,    25,\n",
      "          198, 50280,  7783,  2124,   198, 50284, 17772,    25,   198, 50280,\n",
      "         7783,  2124,  1343,   352,   628,   198,     2,  3551,   257,  2163,\n",
      "          326,  5860,   262,  2160,   286,   262,   717,   299,  3288])\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(completion[0])\n",
    "print(tokenizer.decode(628))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bmo-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f94925f3cff0b0ac90dd538919dcd2d4d02467fa3fdafdf86812e92b857d43c1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
